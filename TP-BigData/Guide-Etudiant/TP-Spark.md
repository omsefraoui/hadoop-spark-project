# TP Apache Spark - Guide √âtudiant

## üìö Objectifs du TP
- Comprendre les concepts de base de Spark (RDD, DataFrame, SQL)
- Manipuler des donn√©es avec PySpark
- Effectuer des analyses de donn√©es distribu√©es
- Utiliser Spark SQL pour requ√™ter des donn√©es

## ‚è±Ô∏è Dur√©e estim√©e
2 heures

---

## üîß Pr√©requis

### Lancement du conteneur Docker
```bash
docker run -it --name hadoop-spark \
  -p 9870:9870 -p 8088:8088 -p 8080:8080 -p 4040:4040 \
  omsefraoui/hadoop-spark-cluster:latest
```

### V√©rification de Spark
Dans le conteneur, v√©rifiez que Spark est bien install√© :
```bash
spark-shell --version
pyspark --version
```

---

## üìñ Partie 1 : Introduction √† PySpark

### Exercice 1.1 : Premier programme Spark
**Objectif** : Cr√©er votre premier RDD et effectuer des transformations de base.

1. Lancez PySpark :
```bash
pyspark
```

2. Cr√©ez un RDD simple :
```python
# Cr√©er un RDD √† partir d'une liste
data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
rdd = sc.parallelize(data)

# Afficher le contenu
print(rdd.collect())
```

3. **Question 1.1** : Appliquez une transformation pour multiplier chaque √©l√©ment par 2.

**Indice** : Utilisez la fonction `map()`

4. **Question 1.2** : Filtrez les nombres pairs uniquement.

**Indice** : Utilisez la fonction `filter()`

5. **Question 1.3** : Calculez la somme de tous les √©l√©ments.

**Indice** : Utilisez la fonction `reduce()`

### Exercice 1.2 : Analyse de texte
**Objectif** : Compter les mots dans un fichier texte.

1. Cr√©ez un fichier texte de test :
```bash
echo "Apache Spark est un framework de traitement de donn√©es" > /tmp/test.txt
echo "Spark permet le traitement distribu√©" >> /tmp/test.txt
echo "Le traitement avec Spark est rapide" >> /tmp/test.txt
```

2. **Question 1.4** : Chargez ce fichier dans un RDD et comptez le nombre total de mots.

```python
# √Ä compl√©ter par l'√©tudiant
text_rdd = sc.textFile("/tmp/test.txt")
# ... votre code ici
```

3. **Question 1.5** : Comptez le nombre d'occurrences de chaque mot (Word Count classique).

**Indice** : Utilisez `flatMap()`, `map()` et `reduceByKey()`

---

## üìä Partie 2 : Spark DataFrames et Spark SQL

### Exercice 2.1 : Cr√©ation et manipulation de DataFrames

**Objectif** : Travailler avec des DataFrames structur√©s.

1. Lancez PySpark avec support Spark SQL :
```bash
pyspark
```

2. Cr√©ez un DataFrame depuis une liste :
```python
from pyspark.sql import SparkSession

# Cr√©er une SparkSession
spark = SparkSession.builder.appName("TP-Spark").getOrCreate()

# Donn√©es d'exemple : √©tudiants
data = [
    (1, "Ahmed", "Informatique", 18),
    (2, "Fatima", "Math√©matiques", 19),
    (3, "Hassan", "Informatique", 20),
    (4, "Samira", "Physique", 18),
    (5, "Omar", "Informatique", 21)
]

# Cr√©er le DataFrame
df = spark.createDataFrame(data, ["id", "nom", "filiere", "age"])
df.show()
```

3. **Question 2.1** : Affichez le sch√©ma du DataFrame.

4. **Question 2.2** : S√©lectionnez uniquement les colonnes "nom" et "filiere".

5. **Question 2.3** : Filtrez les √©tudiants de la fili√®re "Informatique".

6. **Question 2.4** : Comptez le nombre d'√©tudiants par fili√®re.

### Exercice 2.2 : Spark SQL
**Objectif** : Utiliser SQL pour interroger des donn√©es.

1. Cr√©ez une vue temporaire :

```python
df.createOrReplaceTempView("etudiants")
```

2. **Question 2.5** : √âcrivez une requ√™te SQL pour afficher tous les √©tudiants de plus de 18 ans.

```python
result = spark.sql("SELECT ... FROM etudiants WHERE ...")
result.show()
```

3. **Question 2.6** : Calculez l'√¢ge moyen par fili√®re avec SQL.

---

## üìÅ Partie 3 : Analyse de fichier CSV

### Exercice 3.1 : Chargement et analyse de donn√©es CSV
**Objectif** : Analyser un fichier de ventes.

Le fichier `ventes.csv` contient les colonnes suivantes :
- date, produit, categorie, quantite, prix_unitaire, ville

1. **Question 3.1** : Chargez le fichier CSV dans un DataFrame.

```python
df_ventes = spark.read.csv("/data/ventes.csv", header=True, inferSchema=True)
```

2. **Question 3.2** : Affichez les 10 premi√®res lignes et le sch√©ma.

3. **Question 3.3** : Calculez le chiffre d'affaires total (quantite √ó prix_unitaire).

4. **Question 3.4** : Trouvez les 5 produits les plus vendus.

5. **Question 3.5** : Calculez le chiffre d'affaires par cat√©gorie et par ville.

6. **Question 3.6** : Identifiez la ville avec le plus gros chiffre d'affaires.

---

## üéØ Partie 4 : Projet final - Analyse de logs

### Exercice 4.1 : Analyse de logs serveur

**Objectif** : Analyser des logs de serveur web Apache.

Le fichier `/data/logs.txt` contient des logs au format :
```
IP - - [Date] "METHOD /path HTTP/1.1" STATUS SIZE
```

1. **Question 4.1** : Chargez le fichier de logs et comptez le nombre total de requ√™tes.

2. **Question 4.2** : Comptez les requ√™tes par code de statut HTTP (200, 404, 500, etc.).

3. **Question 4.3** : Trouvez les 10 adresses IP les plus actives.

4. **Question 4.4** : Calculez le nombre de requ√™tes par heure de la journ√©e.

5. **Question 4.5** : Identifiez les pages les plus consult√©es (path dans l'URL).

---

## üìù Questions de r√©flexion

1. Quelle est la diff√©rence entre une transformation et une action dans Spark ?

2. Pourquoi utilise-t-on `cache()` ou `persist()` sur un RDD ou DataFrame ?

3. Quels sont les avantages des DataFrames par rapport aux RDDs ?

4. Dans quel cas utiliseriez-vous Spark plut√¥t que du traitement traditionnel ?

---

## üéì Livrable attendu

Cr√©ez un notebook Jupyter ou un script Python contenant :
- Toutes vos r√©ponses aux exercices
- Le code comment√©
- Les r√©sultats d'ex√©cution
- Vos r√©ponses aux questions de r√©flexion

**Format** : `TP_Spark_NOM_Prenom.ipynb` ou `TP_Spark_NOM_Prenom.py`

---

**Bon travail !** üöÄ
