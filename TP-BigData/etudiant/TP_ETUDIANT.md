# TP Big Data : Spark, HBase et Hive
## Travaux Pratiques - Manuel √âtudiant

**Dur√©e :** 4 heures  
**Objectifs :**
- Ma√Ætriser les op√©rations de base avec Apache Spark
- Manipuler des donn√©es NoSQL avec HBase
- Interroger des donn√©es avec Hive et SparkSQL

---

## üìã Pr√©requis

### Installation et d√©marrage du conteneur

1. **T√©l√©charger l'image Docker :**
```bash
docker pull omsefraoui/hadoop-spark-cluster:latest
```

2. **D√©marrer le conteneur :**
```bash
docker run -dit --name bigdata-tp \
  -p 9870:9870 -p 8088:8088 -p 8080:8080 \
  -p 18080:18080 -p 16010:16010 -p 10000:10000 \
  omsefraoui/hadoop-spark-cluster:latest
```

3. **Entrer dans le conteneur :**
```bash
docker exec -it bigdata-tp bash
```

4. **V√©rifier que les services sont d√©marr√©s :**
```bash
# V√©rifier HDFS
hdfs dfs -ls /

# V√©rifier Spark
spark-shell --version

# V√©rifier HBase
hbase shell
> status
> exit

# V√©rifier Hive (si erreur, ex√©cuter /fix-hive.sh)
hive -e "SHOW DATABASES;"
```

---

## üî• PARTIE 1 : Apache Spark (1h30)

### Exercice 1.1 : D√©couverte de PySpark (15 min)

**Objectif :** Cr√©er votre premier programme Spark


**√âtape 1 : Lancer PySpark**
```bash
pyspark
```

**√âtape 2 : Cr√©er un RDD et effectuer des transformations**
```python
# Cr√©er un RDD simple
data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
rdd = sc.parallelize(data)

# Transformation : doubler chaque nombre
rdd_double = rdd.map(lambda x: x * 2)

# Action : afficher les r√©sultats
print(rdd_double.collect())

# Filtrer les nombres pairs
rdd_pairs = rdd.filter(lambda x: x % 2 == 0)
print("Nombres pairs:", rdd_pairs.collect())

# Calculer la somme
total = rdd.reduce(lambda a, b: a + b)
print("Somme:", total)
```

**Questions :**
1. Quelle est la diff√©rence entre une transformation et une action ?
2. Pourquoi utilise-t-on `collect()` ?
3. Modifiez le code pour calculer le carr√© de chaque nombre

---

### Exercice 1.2 : Analyse de fichier texte (20 min)

**Objectif :** Analyser un fichier avec Spark

