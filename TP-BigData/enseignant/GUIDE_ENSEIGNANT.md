# Guide Enseignant - TP Big Data
## Spark, HBase et Hive

---

**Ã‰cole Nationale des Sciences AppliquÃ©es d'Oujda (ENSAO)**  
**FiliÃ¨re** : GÃ©nie Informatique - Big Data  
**DurÃ©e** : 4 heures  
**Niveau** : Master 1/2

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble du TP](#vue-densemble)
2. [Objectifs pÃ©dagogiques](#objectifs-pÃ©dagogiques)
3. [PrÃ©requis et prÃ©paration](#prÃ©requis)
4. [Organisation du TP](#organisation)
5. [BarÃ¨me de notation](#barÃ¨me)
6. [Solutions dÃ©taillÃ©es](#solutions)
7. [Scripts de test automatisÃ©s](#tests)
8. [CritÃ¨res d'Ã©valuation](#critÃ¨res)
9. [DÃ©pannage et FAQ](#dÃ©pannage)

---

## ğŸ¯ Vue d'ensemble du TP {#vue-densemble}

Ce TP permet aux Ã©tudiants de manipuler les trois technologies fondamentales de l'Ã©cosystÃ¨me Big Data :

- **Apache Spark** : Traitement distribuÃ© en mÃ©moire (PySpark)
- **Apache HBase** : Base de donnÃ©es NoSQL orientÃ©e colonnes sur HDFS
- **Apache Hive** : Data warehouse SQL sur Hadoop

### Architecture du TP

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Conteneur Docker Unique             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Spark  â”‚  â”‚ HBase  â”‚  â”‚   Hive   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚           â”‚            â”‚           â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                   â”‚                        â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚            â”‚    HDFS     â”‚                â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dataset utilisÃ©

**ThÃ¨me** : DonnÃ©es de ventes e-commerce  
**Volume** : ~10,000 transactions  
**Format** : CSV, JSON, Parquet

---

## ğŸ“ Objectifs pÃ©dagogiques {#objectifs-pÃ©dagogiques}

### CompÃ©tences visÃ©es

**1. Apache Spark (40% du barÃ¨me)**
- MaÃ®triser l'API PySpark (RDD et DataFrame)
- Effectuer des transformations et actions distribuÃ©es
- Optimiser les requÃªtes avec Spark SQL
- Comprendre le lazy evaluation et les DAG

**2. Apache HBase (30% du barÃ¨me)**
- Concevoir un schÃ©ma HBase (Column Families)
- Manipuler des donnÃ©es avec l'API HappyBase (Python)
- Comprendre le modÃ¨le clÃ©-valeur orientÃ© colonnes
- Effectuer des scans et filtres efficaces

**3. Apache Hive (30% du barÃ¨me)**
- CrÃ©er des tables externes et managÃ©es
- Ã‰crire des requÃªtes HiveQL complexes
- Utiliser les partitions et buckets
- IntÃ©grer Hive avec Spark

### RÃ©sultats d'apprentissage

Ã€ la fin du TP, l'Ã©tudiant sera capable de :
âœ… Choisir la technologie appropriÃ©e selon le cas d'usage
âœ… ImplÃ©menter des pipelines de traitement de donnÃ©es
âœ… Optimiser les performances des requÃªtes Big Data
âœ… Diagnostiquer et corriger les erreurs courantes

---

## ğŸ”§ PrÃ©requis et prÃ©paration {#prÃ©requis}

### Avant le TP

**1. Infrastructure**
```bash
# VÃ©rifier que Docker est installÃ©
docker --version

# Lancer le conteneur
docker run -it --name bigdata-tp \
  -p 9870:9870 -p 8088:8088 -p 8080:8080 -p 16010:16010 -p 10000:10000 \
  omsefraoui/hadoop-spark-cluster:latest
```

**2. MatÃ©riel Ã  distribuer aux Ã©tudiants**
- [ ] Ã‰noncÃ© du TP (ENONCE_ETUDIANT.md)
- [ ] Dataset de ventes (sales_data.csv)
- [ ] Templates de code Python
- [ ] AccÃ¨s au conteneur Docker

**3. VÃ©rification des services**

ExÃ©cuter dans le conteneur :
```bash
# VÃ©rifier HDFS
hdfs dfs -ls /

# VÃ©rifier Spark
spark-submit --version

# VÃ©rifier HBase
hbase shell <<< "status"

# VÃ©rifier Hive
hive -e "SHOW DATABASES;"
```

### Temps de prÃ©paration estimÃ©

- Installation : 15 minutes
- VÃ©rification : 10 minutes
- Distribution du matÃ©riel : 5 minutes

**Total : 30 minutes avant l'arrivÃ©e des Ã©tudiants**

---

## ğŸ“… Organisation du TP {#organisation}

### DÃ©roulement (4 heures)

| Temps | ActivitÃ© | Description |
|-------|----------|-------------|
| 0h00-0h15 | **Introduction** | PrÃ©sentation des objectifs, architecture, dataset |
| 0h15-1h30 | **Partie 1 : Spark** | Exercices 1-5 sur PySpark |
| 1h30-2h30 | **Partie 2 : HBase** | Exercices 6-9 sur HBase |
| 2h30-2h45 | **Pause** | â˜• |
| 2h45-3h45 | **Partie 3 : Hive** | Exercices 10-13 sur Hive |
| 3h45-4h00 | **SynthÃ¨se** | Questions, dÃ©mo des solutions bonus |

### Mode de travail

- **Individuel** ou **BinÃ´mes** (selon effectif)
- **Rendu** : Scripts Python + Rapport PDF
- **Deadline** : Fin de sÃ©ance + 24h pour finaliser le rapport

---

## ğŸ“Š BarÃ¨me de notation {#barÃ¨me}

### Note globale : /20

#### Partie 1 : Apache Spark (8 points)

| Exercice | TÃ¢che | Points | CritÃ¨res |
|----------|-------|--------|----------|
| **Ex1** | Chargement donnÃ©es + RDD de base | 1.5 | - Lecture CSV correcte (0.5)<br>- Affichage Ã©chantillon (0.5)<br>- Count prÃ©cis (0.5) |
| **Ex2** | Transformations RDD (map, filter) | 1.5 | - Extraction prix (0.5)<br>- Filtre > 100â‚¬ (0.5)<br>- RÃ©sultat correct (0.5) |
| **Ex3** | AgrÃ©gations (reduceByKey) | 2.0 | - GroupBy catÃ©gorie (0.7)<br>- Calcul CA par catÃ©gorie (0.7)<br>- Top 3 catÃ©gories (0.6) |
| **Ex4** | DataFrame API + Spark SQL | 2.0 | - Conversion DataFrame (0.5)<br>- RequÃªte SQL correcte (1.0)<br>- RÃ©sultats justes (0.5) |
| **Ex5** | Optimisation + Cache | 1.0 | - Utilisation de cache() (0.5)<br>- Explication pertinente (0.5) |

**Bonus Spark** : +0.5 pour utilisation de fenÃªtres (window functions)

#### Partie 2 : Apache HBase (6 points)

| Exercice | TÃ¢che | Points | CritÃ¨res |
|----------|-------|--------|----------|
| **Ex6** | CrÃ©ation table + Column Families | 1.5 | - SchÃ©ma pertinent (0.7)<br>- CFs bien dÃ©finies (0.5)<br>- Table crÃ©Ã©e (0.3) |
| **Ex7** | Insertion de donnÃ©es | 1.5 | - 100+ lignes insÃ©rÃ©es (0.7)<br>- Format row key correct (0.5)<br>- DonnÃ©es cohÃ©rentes (0.3) |
| **Ex8** | Lecture et scan | 1.5 | - Get by key (0.5)<br>- Scan avec filtre (0.7)<br>- RÃ©sultats corrects (0.3) |
| **Ex9** | RequÃªtes avancÃ©es | 1.5 | - Scan avec prÃ©fixe (0.5)<br>- Filtre temporel (0.5)<br>- AgrÃ©gation (0.5) |

**Bonus HBase** : +0.5 pour implÃ©mentation de compteurs (increment)

#### Partie 3 : Apache Hive (6 points)

| Exercice | TÃ¢che | Points | CritÃ¨res |
|----------|-------|--------|----------|
| **Ex10** | CrÃ©ation table externe | 1.5 | - DDL correct (0.7)<br>- Partition dÃ©finie (0.5)<br>- Chargement donnÃ©es (0.3) |
| **Ex11** | RequÃªtes HiveQL | 2.0 | - SELECT avec JOIN (0.7)<br>- AgrÃ©gations (0.7)<br>- RÃ©sultats justes (0.6) |
| **Ex12** | Partitionnement | 1.5 | - Partition dynamique (0.7)<br>- RequÃªte sur partition (0.5)<br>- Performance amÃ©liorÃ©e (0.3) |
| **Ex13** | IntÃ©gration Spark-Hive | 1.0 | - Lecture table Hive via Spark (0.5)<br>- Transformation + Ã‰criture (0.5) |

**Bonus Hive** : +0.5 pour utilisation de buckets

### Rapport et qualitÃ© du code (/20)

- **ClartÃ© du code** : 2 points (commentaires, nommage)
- **Rapport technique** : 2 points (explications, schÃ©mas)
- **Gestion des erreurs** : 1 point (try-except, validation)
- **OriginalitÃ©** : 1 point (approches crÃ©atives)

**Total maximum : 20 points + 1.5 bonus = 21.5 â†’ ramenÃ© Ã  20/20**

---

## âœ… Solutions dÃ©taillÃ©es {#solutions}

### PARTIE 1 : Apache Spark

#### Exercice 1 : Chargement et exploration des donnÃ©es (1.5 pts)

**Ã‰noncÃ©** :  
Charger le fichier `sales_data.csv` dans un RDD Spark, afficher les 5 premiÃ¨res lignes et compter le nombre total de transactions.

**Solution complÃ¨te** :

```python
from pyspark.sql import SparkSession

# Initialisation Spark
spark = SparkSession.builder \
    .appName("TP BigData - Spark") \
    .master("local[*]") \
    .getOrCreate()

sc = spark.sparkContext

# Chargement du fichier CSV en RDD
rdd = sc.textFile("hdfs:///data/sales_data.csv")

# Afficher les 5 premiÃ¨res lignes
print("=== 5 premiÃ¨res lignes ===")
for ligne in rdd.take(5):
    print(ligne)

# Compter le nombre total de lignes (- 1 pour l'en-tÃªte)
header = rdd.first()
rdd_sans_header = rdd.filter(lambda ligne: ligne != header)
nb_transactions = rdd_sans_header.count()

print(f"\nNombre total de transactions : {nb_transactions}")
```

**RÃ©sultat attendu** :
```
=== 5 premiÃ¨res lignes ===
transaction_id,date,customer_id,product_id,category,quantity,price,payment_method
TX001,2024-01-15,CUST123,PROD456,Electronics,2,599.99,credit_card
TX002,2024-01-15,CUST124,PROD789,Clothing,1,49.99,paypal
...

Nombre total de transactions : 9842
```

**Points d'Ã©valuation** :
- âœ… 0.5 pt : Lecture CSV correcte avec `textFile()`
- âœ… 0.5 pt : Affichage des 5 premiÃ¨res lignes avec `take()`
- âœ… 0.5 pt : Count prÃ©cis (exclusion de l'en-tÃªte)


**Erreurs courantes des Ã©tudiants** :
- âŒ Ne pas supprimer l'en-tÃªte avant le count
- âŒ Utiliser `collect()` au lieu de `take()` (problÃ¨me mÃ©moire)
- âŒ Ne pas gÃ©rer le sÃ©parateur CSV

---

#### Exercice 2 : Transformations RDD (1.5 pts)

**Ã‰noncÃ©** :  
Extraire les prix des transactions et filtrer celles dont le montant est supÃ©rieur Ã  100â‚¬. Calculer le prix moyen.

**Solution complÃ¨te** :

```python
# Parser une ligne CSV
def parse_ligne(ligne):
    """Parse une ligne CSV et retourne un dictionnaire"""
    champs = ligne.split(',')
    return {
        'transaction_id': champs[0],
        'date': champs[1],
        'customer_id': champs[2],
        'product_id': champs[3],
        'category': champs[4],
        'quantity': int(champs[5]),
        'price': float(champs[6]),
        'payment_method': champs[7]
    }

# RDD sans header
rdd_data = rdd_sans_header.map(parse_ligne)

# Extraire les prix
rdd_prix = rdd_data.map(lambda x: x['price'])

print("=== 5 premiers prix ===")
for prix in rdd_prix.take(5):
    print(f"{prix}â‚¬")

# Filtrer les transactions > 100â‚¬
rdd_prix_eleves = rdd_prix.filter(lambda prix: prix > 100)

nb_transactions_elevees = rdd_prix_eleves.count()
prix_moyen = rdd_prix_eleves.mean()

print(f"\nTransactions > 100â‚¬ : {nb_transactions_elevees}")
print(f"Prix moyen de ces transactions : {prix_moyen:.2f}â‚¬")
```

**RÃ©sultat attendu** :
```
=== 5 premiers prix ===
599.99â‚¬
49.99â‚¬
1299.99â‚¬
...

Transactions > 100â‚¬ : 3542
Prix moyen de ces transactions : 487.63â‚¬
```

**Points d'Ã©valuation** :
- âœ… 0.5 pt : Extraction correcte des prix avec `map()`
- âœ… 0.5 pt : Filtre `> 100` avec `filter()`
- âœ… 0.5 pt : Calcul du prix moyen avec `mean()`

---

#### Exercice 3 : AgrÃ©gations par clÃ© (2.0 pts)

**Ã‰noncÃ©** :  
Calculer le chiffre d'affaires (CA) par catÃ©gorie de produits et afficher le top 3 des catÃ©gories les plus rentables.

**Solution complÃ¨te** :

```python
# RDD de paires (catÃ©gorie, montant_total)
# montant_total = quantity * price
rdd_ca = rdd_data.map(lambda x: (x['category'], x['quantity'] * x['price']))

# AgrÃ©gation par catÃ©gorie
ca_par_categorie = rdd_ca.reduceByKey(lambda a, b: a + b)

print("=== Chiffre d'affaires par catÃ©gorie ===")
for categorie, ca in ca_par_categorie.collect():
    print(f"{categorie}: {ca:.2f}â‚¬")

# Top 3 catÃ©gories
top3_categories = ca_par_categorie.sortBy(lambda x: x[1], ascending=False).take(3)

print("\n=== TOP 3 CatÃ©gories par CA ===")
for i, (categorie, ca) in enumerate(top3_categories, 1):
    print(f"{i}. {categorie}: {ca:.2f}â‚¬")
```

**RÃ©sultat attendu** :
```
=== Chiffre d'affaires par catÃ©gorie ===
Electronics: 1245789.50â‚¬
Clothing: 543210.30â‚¬
Books: 234567.80â‚¬
Home: 678901.20â‚¬
Sports: 345678.90â‚¬

=== TOP 3 CatÃ©gories par CA ===
1. Electronics: 1245789.50â‚¬
2. Home: 678901.20â‚¬
3. Clothing: 543210.30â‚¬
```

**Points d'Ã©valuation** :
- âœ… 0.7 pt : CrÃ©ation paires (catÃ©gorie, montant) avec map()
- âœ… 0.7 pt : AgrÃ©gation correcte avec reduceByKey()
- âœ… 0.6 pt : Top 3 avec sortBy() et take()

**Erreurs courantes** :
- âŒ Utiliser `groupByKey()` au lieu de `reduceByKey()` (moins performant)
- âŒ Oublier de multiplier quantity Ã— price
- âŒ Tri dans le mauvais ordre (ascending au lieu de descending)

---

#### Exercice 4 : DataFrame API et Spark SQL (2.0 pts)

**Ã‰noncÃ©** :  
Convertir le RDD en DataFrame et utiliser Spark SQL pour trouver le client ayant dÃ©pensÃ© le plus.

**Solution complÃ¨te** :

```python
from pyspark.sql import Row

# Conversion RDD â†’ DataFrame
df = rdd_data.map(lambda x: Row(**x)).toDF()

# Afficher le schÃ©ma
df.printSchema()
df.show(5)

# CrÃ©er une vue temporaire pour SQL
df.createOrReplaceTempView("sales")

# RequÃªte SQL : client ayant le plus dÃ©pensÃ©
query = """
SELECT 
    customer_id,
    SUM(quantity * price) as total_depense,
    COUNT(*) as nb_transactions
FROM sales
GROUP BY customer_id
ORDER BY total_depense DESC
LIMIT 1
"""

meilleur_client = spark.sql(query)
meilleur_client.show()

# Alternative avec DataFrame API
from pyspark.sql.functions import sum, count, col

meilleur_client_df = df \
    .withColumn("montant", col("quantity") * col("price")) \
    .groupBy("customer_id") \
    .agg(
        sum("montant").alias("total_depense"),
        count("*").alias("nb_transactions")
    ) \
    .orderBy(col("total_depense").desc()) \
    .limit(1)

meilleur_client_df.show()
```

**RÃ©sultat attendu** :
```
+-----------+--------------+----------------+
|customer_id|total_depense |nb_transactions |
+-----------+--------------+----------------+
|CUST742    |15789.45      |47              |
+-----------+--------------+----------------+
```

**Points d'Ã©valuation** :
- âœ… 0.5 pt : Conversion RDD â†’ DataFrame correcte
- âœ… 1.0 pt : RequÃªte SQL fonctionnelle (GROUP BY, SUM, ORDER BY)
- âœ… 0.5 pt : RÃ©sultat correct (meilleur client identifiÃ©)

---

#### Exercice 5 : Optimisation avec Cache (1.0 pt)

**Ã‰noncÃ©** :  
DÃ©montrer l'utilisation du cache pour optimiser les requÃªtes rÃ©pÃ©tÃ©es sur le mÃªme DataFrame.

**Solution complÃ¨te** :

```python
import time

# Sans cache
start = time.time()
count1 = df.filter(df.price > 500).count()
count2 = df.filter(df.price > 500).count()
temps_sans_cache = time.time() - start

print(f"Temps sans cache : {temps_sans_cache:.2f}s")

# Avec cache
df_cache = df.cache()

start = time.time()
count1 = df_cache.filter(df_cache.price > 500).count()
count2 = df_cache.filter(df_cache.price > 500).count()
temps_avec_cache = time.time() - start

print(f"Temps avec cache : {temps_avec_cache:.2f}s")
print(f"Gain de performance : {(temps_sans_cache - temps_avec_cache) / temps_sans_cache * 100:.1f}%")

# LibÃ©rer le cache
df_cache.unpersist()
```

**RÃ©sultat attendu** :
```
Temps sans cache : 2.45s
Temps avec cache : 1.12s
Gain de performance : 54.3%
```

**Points d'Ã©valuation** :
- âœ… 0.5 pt : Utilisation correcte de `cache()`
- âœ… 0.5 pt : Explication pertinente du gain de performance

**Explication attendue** :  
Le cache stocke le DataFrame en mÃ©moire aprÃ¨s la premiÃ¨re exÃ©cution. Les requÃªtes suivantes n'ont pas besoin de relire le fichier source, d'oÃ¹ le gain de performance notable (~50%).

---

### PARTIE 2 : Apache HBase

#### Exercice 6 : CrÃ©ation de table et Column Families (1.5 pts)

**Ã‰noncÃ©** :  
CrÃ©er une table HBase pour stocker les ventes avec un schÃ©ma orientÃ© colonnes optimisÃ©.

**Solution complÃ¨te** :

```python
import happybase

# Connexion Ã  HBase
connection = happybase.Connection('localhost', port=9090)
print("Connexion HBase Ã©tablie")

# CrÃ©er la table avec Column Families
table_name = 'sales'

# Supprimer la table si elle existe dÃ©jÃ 
if table_name.encode() in connection.tables():
    connection.delete_table(table_name, disable=True)
    print(f"Table {table_name} supprimÃ©e")

# DÃ©finir les Column Families
families = {
    'info': dict(),           # Informations gÃ©nÃ©rales (date, customer_id)
    'product': dict(),        # DÃ©tails produit (product_id, category)
    'transaction': dict()     # DÃ©tails transaction (quantity, price, payment)
}

# CrÃ©er la table
connection.create_table(table_name, families)
print(f"Table {table_name} crÃ©Ã©e avec succÃ¨s")

# VÃ©rifier que la table existe
tables = [t.decode() for t in connection.tables()]
print(f"\nTables disponibles : {tables}")

# Afficher le descripteur de la table
table = connection.table(table_name)
print(f"\nColumn Families de '{table_name}' : {list(table.families())}")
```

**RÃ©sultat attendu** :
```
Connexion HBase Ã©tablie
Table sales supprimÃ©e
Table sales crÃ©Ã©e avec succÃ¨s

Tables disponibles : ['sales']

Column Families de 'sales' : [b'info', b'product', b'transaction']
```

**Points d'Ã©valuation** :
- âœ… 0.7 pt : SchÃ©ma pertinent (3 CF logiquement sÃ©parÃ©es)
- âœ… 0.5 pt : Column Families bien dÃ©finies (info, product, transaction)
- âœ… 0.3 pt : Table crÃ©Ã©e sans erreur

**Justification du schÃ©ma** :
- `info:` donnÃ©es temporelles et client (faible volumÃ©trie)
- `product:` informations produit (modÃ©rÃ©e volumÃ©trie)
- `transaction:` dÃ©tails transactionnels (haute frÃ©quence d'accÃ¨s)

---

#### Exercice 7 : Insertion de donnÃ©es (1.5 pts)

**Ã‰noncÃ©** :  
InsÃ©rer au moins 100 transactions dans la table HBase avec des row keys optimisÃ©es.

**Solution complÃ¨te** :

```python
import csv
from datetime import datetime

# Fonction pour gÃ©nÃ©rer une row key optimisÃ©e
def generate_row_key(transaction_id, date):
    """
    Row Key format: YYYYMMDD_TransactionID
    Permet un bon Ã©quilibrage et des scans efficaces par date
    """
    date_str = date.replace('-', '')  # 2024-01-15 â†’ 20240115
    return f"{date_str}_{transaction_id}"

# Charger les donnÃ©es CSV
with open('/data/sales_data.csv', 'r') as f:
    reader = csv.DictReader(f)
    data = list(reader)

# Insertion batch
table = connection.table('sales')
batch = table.batch(batch_size=100)

print("Insertion des donnÃ©es...")
count = 0

for row in data[:100]:  # InsÃ©rer les 100 premiÃ¨res
    row_key = generate_row_key(row['transaction_id'], row['date'])
    
    batch.put(row_key.encode(), {
        b'info:date': row['date'].encode(),
        b'info:customer_id': row['customer_id'].encode(),
        b'product:product_id': row['product_id'].encode(),
        b'product:category': row['category'].encode(),
        b'transaction:quantity': row['quantity'].encode(),
        b'transaction:price': row['price'].encode(),
        b'transaction:payment_method': row['payment_method'].encode()
    })
    
    count += 1
    if count % 20 == 0:
        print(f"  {count} lignes insÃ©rÃ©es...")

batch.send()
print(f"\nâœ“ Total : {count} transactions insÃ©rÃ©es")

# VÃ©rification
nb_lignes = sum(1 for _ in table.scan())
print(f"Nombre de lignes dans la table : {nb_lignes}")
```

**RÃ©sultat attendu** :
```
Insertion des donnÃ©es...
  20 lignes insÃ©rÃ©es...
  40 lignes insÃ©rÃ©es...
  60 lignes insÃ©rÃ©es...
  80 lignes insÃ©rÃ©es...
  100 lignes insÃ©rÃ©es...

âœ“ Total : 100 transactions insÃ©rÃ©es
Nombre de lignes dans la table : 100
```

**Points d'Ã©valuation** :
- âœ… 0.7 pt : 100+ lignes insÃ©rÃ©es avec batch
- âœ… 0.5 pt : Format row key optimisÃ© (date_transaction_id)
- âœ… 0.3 pt : DonnÃ©es cohÃ©rentes dans les 3 CF

**Row Key Design - Bonnes pratiques** :
- âœ… PrÃ©fixer par date pour scans temporels efficaces
- âœ… Ã‰viter les hotspots (pas de timestamp monotone)
- âœ… Inclure un identifiant unique

---

#### Exercice 8 : Lecture et scan (1.5 pts)

**Ã‰noncÃ©** :  
RÃ©cupÃ©rer une transaction par sa clÃ©, puis scanner toutes les transactions d'une catÃ©gorie spÃ©cifique.

**Solution complÃ¨te** :

```python
# 1. Get by row key
print("=== Lecture d'une transaction spÃ©cifique ===")
row_key = b'20240115_TX001'
row = table.row(row_key)

if row:
    print(f"Transaction {row_key.decode()} :")
    for key, value in row.items():
        print(f"  {key.decode()}: {value.decode()}")
else:
    print("Transaction non trouvÃ©e")


# 2. Scan avec filtre par catÃ©gorie
print("\n=== Scan des transactions 'Electronics' ===")
categorie = b'Electronics'
count_electronics = 0

for key, data in table.scan():
    if data.get(b'product:category') == categorie:
        count_electronics += 1
        if count_electronics <= 5:  # Afficher les 5 premiÃ¨res
            print(f"{key.decode()}: {data[b'transaction:price'].decode()}â‚¬")

print(f"\nTotal transactions Electronics : {count_electronics}")
```

**RÃ©sultat attendu** :
```
=== Lecture d'une transaction spÃ©cifique ===
Transaction 20240115_TX001 :
  info:date: 2024-01-15
  info:customer_id: CUST123
  product:product_id: PROD456
  product:category: Electronics
  transaction:quantity: 2
  transaction:price: 599.99
  transaction:payment_method: credit_card

=== Scan des transactions 'Electronics' ===
20240115_TX001: 599.99â‚¬
20240115_TX005: 1299.99â‚¬
20240116_TX012: 799.00â‚¬
20240117_TX023: 449.50â‚¬
20240118_TX034: 899.99â‚¬

Total transactions Electronics : 28
```

**Points d'Ã©valuation** :
- âœ… 0.5 pt : Get by key avec `table.row()`
- âœ… 0.7 pt : Scan avec filtre sur category
- âœ… 0.3 pt : RÃ©sultats corrects affichÃ©s

---

#### Exercice 9 : RequÃªtes avancÃ©es (1.5 pts)

**Ã‰noncÃ©** :  
Scanner les transactions d'une pÃ©riode donnÃ©e et calculer le total des ventes.


**Solution complÃ¨te** :

```python
# Scanner les transactions du 15 au 20 janvier 2024
print("=== Transactions du 15 au 20 janvier 2024 ===")

start_row = b'20240115'  # DÃ©but de pÃ©riode
stop_row = b'20240121'   # Fin de pÃ©riode (exclusif)

total_ventes = 0
total_quantite = 0
count = 0

for key, data in table.scan(row_start=start_row, row_stop=stop_row):
    prix = float(data[b'transaction:price'].decode())
    quantite = int(data[b'transaction:quantity'].decode())
    montant = prix * quantite
    
    total_ventes += montant
    total_quantite += quantite
    count += 1
    
    if count <= 3:  # Afficher les 3 premiÃ¨res
        date = data[b'info:date'].decode()
        print(f"{date} - {key.decode()}: {quantite} Ã— {prix}â‚¬ = {montant}â‚¬")

print(f"\nğŸ“Š Statistiques pÃ©riode 15-20 janvier :")
print(f"  - Transactions : {count}")
print(f"  - QuantitÃ© totale : {total_quantite}")
print(f"  - CA total : {total_ventes:.2f}â‚¬")
print(f"  - Panier moyen : {total_ventes/count:.2f}â‚¬")
```

**RÃ©sultat attendu** :
```
=== Transactions du 15 au 20 janvier 2024 ===
2024-01-15 - 20240115_TX001: 2 Ã— 599.99â‚¬ = 1199.98â‚¬
2024-01-15 - 20240115_TX002: 1 Ã— 49.99â‚¬ = 49.99â‚¬
2024-01-16 - 20240116_TX010: 3 Ã— 29.99â‚¬ = 89.97â‚¬

ğŸ“Š Statistiques pÃ©riode 15-20 janvier :
  - Transactions : 42
  - QuantitÃ© totale : 87
  - CA total : 18745.63â‚¬
  - Panier moyen : 446.32â‚¬
```

**Points d'Ã©valuation** :
- âœ… 0.5 pt : Scan avec row_start et row_stop
- âœ… 0.5 pt : Calcul du CA (quantity Ã— price)
- âœ… 0.5 pt : AgrÃ©gations correctes (sum, count, avg)

**Optimisation HBase** :
Le prÃ©fixage des row keys par date (YYYYMMDD) permet des scans temporels trÃ¨s efficaces sans nÃ©cessiter de filtre cÃ´tÃ© application.

---

### PARTIE 3 : Apache Hive

#### Exercice 10 : CrÃ©ation de table externe (1.5 pts)

**Ã‰noncÃ©** :  
CrÃ©er une table externe Hive pointant vers les donnÃ©es CSV sur HDFS, avec partitionnement par date.

**Solution complÃ¨te** :

```python
from pyhive import hive

# Connexion Ã  Hive
conn = hive.Connection(host='localhost', port=10000, username='hadoop')
cursor = conn.cursor()

print("Connexion Ã  Hive Ã©tablie")

# 1. CrÃ©er une base de donnÃ©es
cursor.execute("CREATE DATABASE IF NOT EXISTS sales_db")
cursor.execute("USE sales_db")
print("Base de donnÃ©es 'sales_db' crÃ©Ã©e/sÃ©lectionnÃ©e")


# 2. CrÃ©er une table externe (non partitionnÃ©e)
create_table_sql = """
CREATE EXTERNAL TABLE IF NOT EXISTS sales_raw (
    transaction_id STRING,
    date STRING,
    customer_id STRING,
    product_id STRING,
    category STRING,
    quantity INT,
    price DOUBLE,
    payment_method STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/data/sales/'
TBLPROPERTIES ('skip.header.line.count'='1')
"""

cursor.execute(create_table_sql)
print("Table 'sales_raw' crÃ©Ã©e")

# 3. CrÃ©er une table partitionnÃ©e
create_partitioned_sql = """
CREATE TABLE IF NOT EXISTS sales_partitioned (
    transaction_id STRING,
    customer_id STRING,
    product_id STRING,
    category STRING,
    quantity INT,
    price DOUBLE,
    payment_method STRING
)
PARTITIONED BY (date STRING)
STORED AS PARQUET
"""

cursor.execute(create_partitioned_sql)
print("Table 'sales_partitioned' crÃ©Ã©e")


# 4. Charger les donnÃ©es avec partition dynamique
cursor.execute("SET hive.exec.dynamic.partition = true")
cursor.execute("SET hive.exec.dynamic.partition.mode = nonstrict")

insert_sql = """
INSERT OVERWRITE TABLE sales_partitioned PARTITION(date)
SELECT 
    transaction_id,
    customer_id,
    product_id,
    category,
    quantity,
    price,
    payment_method,
    date
FROM sales_raw
"""

cursor.execute(insert_sql)
print("DonnÃ©es chargÃ©es dans la table partitionnÃ©e")

# VÃ©rification
cursor.execute("SHOW PARTITIONS sales_partitioned")
partitions = cursor.fetchall()
print(f"\nPartitions crÃ©Ã©es : {len(partitions)}")
for p in partitions[:5]:
    print(f"  - {p[0]}")
```

**RÃ©sultat attendu** :
```
Connexion Ã  Hive Ã©tablie
Base de donnÃ©es 'sales_db' crÃ©Ã©e/sÃ©lectionnÃ©e
Table 'sales_raw' crÃ©Ã©e
Table 'sales_partitioned' crÃ©Ã©e
DonnÃ©es chargÃ©es dans la table partitionnÃ©e

Partitions crÃ©Ã©es : 15
  - date=2024-01-15
  - date=2024-01-16
  - date=2024-01-17
  - date=2024-01-18
  - date=2024-01-19
```

**Points d'Ã©valuation** :
- âœ… 0.7 pt : DDL correct (CREATE EXTERNAL TABLE)
- âœ… 0.5 pt : Partition dÃ©finie (PARTITIONED BY)
- âœ… 0.3 pt : Chargement donnÃ©es rÃ©ussi

---

#### Exercice 11 : RequÃªtes HiveQL complexes (2.0 pts)

**Ã‰noncÃ©** :  
Ã‰crire une requÃªte HiveQL pour analyser les ventes par catÃ©gorie et mÃ©thode de paiement.

**Solution complÃ¨te** :

```python
# RequÃªte 1 : CA par catÃ©gorie et mÃ©thode de paiement
query1 = """
SELECT 
    category,
    payment_method,
    COUNT(*) as nb_transactions,
    SUM(quantity * price) as ca_total,
    AVG(quantity * price) as panier_moyen
FROM sales_partitioned
WHERE date BETWEEN '2024-01-15' AND '2024-01-20'
GROUP BY category, payment_method
ORDER BY ca_total DESC
LIMIT 10
"""

cursor.execute(query1)
results = cursor.fetchall()

print("=== CA par catÃ©gorie et mÃ©thode de paiement ===")
print(f"{'CatÃ©gorie':<15} {'Paiement':<15} {'Nb Trans':<10} {'CA Total':<15} {'Panier Moy':<15}")
print("-" * 75)
for row in results:
    print(f"{row[0]:<15} {row[1]:<15} {row[2]:<10} {row[3]:<15.2f} {row[4]:<15.2f}")
