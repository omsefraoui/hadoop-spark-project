# TP Big Data - Guide de l'Enseignant
## Apache Spark, HBase et Hive

**Niveau :** Master 1/2 - √âcole d'Ing√©nieurs  
**Dur√©e :** 4 heures  
**Auteur :** Omar Sefraoui - ENSAO

---

## Table des Mati√®res

1. [Vue d'ensemble du TP](#1-vue-densemble-du-tp)
2. [Objectifs p√©dagogiques](#2-objectifs-p√©dagogiques)
3. [Pr√©requis techniques](#3-pr√©requis-techniques)
4. [Organisation de la s√©ance](#4-organisation-de-la-s√©ance)
5. [Solutions d√©taill√©es](#5-solutions-d√©taill√©es)
6. [Bar√®mes de notation](#6-bar√®mes-de-notation)
7. [Scripts de test automatis√©s](#7-scripts-de-test-automatis√©s)
8. [Crit√®res d'√©valuation](#8-crit√®res-d√©valuation)
9. [Probl√®mes courants et solutions](#9-probl√®mes-courants-et-solutions)

---

## 1. Vue d'ensemble du TP

Ce TP pratique permet aux √©tudiants de manipuler trois technologies Big Data compl√©mentaires :

- **Apache Spark** : Traitement distribu√© de donn√©es (batch)
- **Apache HBase** : Base de donn√©es NoSQL orient√©e colonnes
- **Apache Hive** : Data Warehouse SQL sur Hadoop

### Architecture du cluster

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Container Docker (All-in-One)       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ  Hadoop  ‚îÇ  ‚îÇ  Spark   ‚îÇ  ‚îÇ  Hive  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  HDFS    ‚îÇ  ‚îÇ  Engine  ‚îÇ  ‚îÇ  Meta  ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ       ‚îÇ           ‚îÇ            ‚îÇ        ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ                   ‚îÇ                      ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ              ‚îÇ  HBase   ‚îÇ                ‚îÇ
‚îÇ              ‚îÇ RegionSvr‚îÇ                ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Sc√©nario p√©dagogique

Les √©tudiants travaillent sur un cas d'usage r√©aliste : **Analyse de donn√©es de ventes e-commerce**.

Ils vont :
1. Charger et analyser des logs de ventes avec **Spark**
2. Stocker des profils clients en temps r√©el avec **HBase**
3. Cr√©er un entrep√¥t de donn√©es analytiques avec **Hive**

---

## 2. Objectifs p√©dagogiques

### Comp√©tences vis√©es

**Spark (35% du TP)**
- Ma√Ætriser les RDD et DataFrames
- √âcrire des transformations et actions
- Effectuer des agr√©gations complexes
- Optimiser les performances (cache, partitionnement)

**HBase (30% du TP)**
- Comprendre le mod√®le cl√©-valeur orient√© colonnes
- Concevoir des sch√©mas de tables efficaces
- Manipuler les op√©rations CRUD
- Utiliser les scans et filtres

**Hive (35% du TP)**
- Cr√©er des tables internes et externes
- √âcrire des requ√™tes SQL complexes (JOIN, GROUP BY, window functions)
- Partitionner et bucketer des tables
- Int√©grer Hive avec Spark

### R√©sultats d'apprentissage attendus

√Ä la fin du TP, l'√©tudiant doit √™tre capable de :
- ‚úÖ Choisir la technologie adapt√©e selon le cas d'usage
- ‚úÖ Impl√©menter un pipeline de traitement de donn√©es
- ‚úÖ Optimiser les requ√™tes et performances
- ‚úÖ D√©boguer les erreurs courantes

---

## 3. Pr√©requis techniques

### C√¥t√© √©tudiant
- **Docker Desktop** install√© et fonctionnel
- **Python 3.8+** avec pip
- √âditeur de code (VS Code recommand√©)
- 8 GB RAM minimum, 16 GB recommand√©
- 20 GB espace disque libre

### C√¥t√© enseignant
- Image Docker : `omsefraoui/hadoop-spark-cluster:latest`
- Scripts de test Python fournis
- Acc√®s aux logs des conteneurs
- Bar√®me de notation d√©taill√©

---

## 4. Organisation de la s√©ance

### Timeline recommand√©e (4h)

| Temps | Activit√© | Dur√©e |
|-------|----------|-------|
| 0:00 | Introduction + Installation | 30 min |
| 0:30 | **Partie 1 : Spark** | 90 min |
| 2:00 | Pause | 15 min |
| 2:15 | **Partie 2 : HBase** | 60 min |
| 3:15 | **Partie 3 : Hive** | 45 min |

### D√©marrage du cluster (√† faire ensemble)

**Commande de d√©marrage :**
```bash
docker run -d --name bigdata-cluster \
  -p 9870:9870 -p 8088:8088 -p 8080:8080 \
  -p 16010:16010 -p 10000:10000 \
  omsefraoui/hadoop-spark-cluster:latest
```

**V√©rification des services :**
```bash
docker exec bigdata-cluster jps
```

**R√©sultat attendu :**
```
NameNode
DataNode
ResourceManager
NodeManager
HMaster
HRegionServer
RunJar (Hive services)
```

**URLs d'acc√®s (√† afficher au tableau) :**
- HDFS NameNode : http://localhost:9870
- YARN ResourceManager : http://localhost:8088
- Spark Master : http://localhost:8080
- HBase Master : http://localhost:16010

---

## 5. Solutions d√©taill√©es

### PARTIE 1 : Apache Spark (90 minutes)

#### Exercice 1.1 : Premiers pas avec Spark (15 min) - 3 points

**√ânonc√© :**
> Cr√©ez un fichier `ventes.csv` avec 10 transactions et chargez-le dans Spark.

**Solution compl√®te :**

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *

# Initialisation de la session Spark
spark = SparkSession.builder \
    .appName("TP_Ventes") \
    .master("local[*]") \
    .getOrCreate()

# Cr√©ation des donn√©es de test
data = [
    ("TXN001", "2024-01-15", "Laptop", 1200.00, "Paris", "Alice"),
    ("TXN002", "2024-01-15", "Mouse", 25.50, "Lyon", "Bob"),
    ("TXN003", "2024-01-16", "Keyboard", 75.00, "Paris", "Alice"),
    ("TXN004", "2024-01-16", "Monitor", 350.00, "Marseille", "Charlie"),
    ("TXN005", "2024-01-17", "Laptop", 1200.00, "Lyon", "Bob"),
    ("TXN006", "2024-01-17", "Mouse", 25.50, "Paris", "Alice"),
    ("TXN007", "2024-01-18", "Laptop", 1400.00, "Paris", "David"),
    ("TXN008", "2024-01-18", "Keyboard", 75.00, "Lyon", "Bob"),
    ("TXN009", "2024-01-19", "Monitor", 350.00, "Paris", "Alice"),
    ("TXN010", "2024-01-19", "Mouse", 30.00, "Marseille", "Charlie")
]

# Sch√©ma explicite (BONNE PRATIQUE)
schema = StructType([
    StructField("transaction_id", StringType(), False),
    StructField("date", StringType(), False),
    StructField("produit", StringType(), False),
    StructField("montant", DoubleType(), False),
    StructField("ville", StringType(), False),
    StructField("client", StringType(), False)
])

# Cr√©ation du DataFrame
df = spark.createDataFrame(data, schema)

# Affichage
df.show()
df.printSchema()

# Sauvegarde en CSV
df.write.mode("overwrite").csv("/tmp/ventes.csv", header=True)

print("‚úÖ Donn√©es charg√©es avec succ√®s !")
print(f"Nombre de lignes : {df.count()}")
```

**Bar√®me d√©taill√© (3 points) :**
- 0.5 pt : Session Spark correctement initialis√©e
- 1.0 pt : Donn√©es cr√©√©es avec au moins 10 lignes
- 0.5 pt : Sch√©ma explicite d√©fini
- 0.5 pt : Affichage correct (show + printSchema)
- 0.5 pt : Code propre et comment√©

**Points de vigilance :**
- ‚ö†Ô∏è V√©rifier que le sch√©ma est explicite (pas d'inf√©rence)
- ‚ö†Ô∏è Les types de donn√©es doivent √™tre corrects (DoubleType pour montant)
- ‚ö†Ô∏è Pr√©sence de `header=True` lors de la sauvegarde

---

#### Exercice 1.2 : Analyses statistiques (20 min) - 4 points

**√ânonc√© :**
> Calculez les statistiques suivantes :
> 1. Chiffre d'affaires total
> 2. Ventes par ville
> 3. Top 3 des produits
> 4. Client avec le plus de transactions

**Solution compl√®te :**

```python
from pyspark.sql.functions import sum, count, desc

# 1. Chiffre d'affaires total
ca_total = df.agg(sum("montant").alias("CA_Total")).collect()[0]["CA_Total"]
print(f"üìä Chiffre d'affaires total : {ca_total:.2f} ‚Ç¨")

# 2. Ventes par ville
print("\nüìç Ventes par ville :")
df.groupBy("ville") \
    .agg(
        sum("montant").alias("CA"),
        count("*").alias("Nb_Transactions")
    ) \
    .orderBy(desc("CA")) \
    .show()

# 3. Top 3 des produits
print("\nüèÜ Top 3 des produits :")
df.groupBy("produit") \
    .agg(
        sum("montant").alias("CA"),
        count("*").alias("Ventes")
    ) \
    .orderBy(desc("CA")) \
    .limit(3) \
    .show()

# 4. Client avec le plus de transactions
print("\nüë§ Client le plus actif :")
df.groupBy("client") \
    .agg(
        count("*").alias("Nb_Achats"),
        sum("montant").alias("CA_Total")
    ) \
    .orderBy(desc("Nb_Achats")) \
    .limit(1) \
    .show()
```

**R√©sultat attendu :**
```
üìä Chiffre d'affaires total : 4506.00 ‚Ç¨

üìç Ventes par ville :
+----------+-------+----------------+
|     ville|     CA|Nb_Transactions|
+----------+-------+----------------+
|     Paris|3080.50|               5|
|      Lyon|1326.00|               3|
|Marseille| 380.00|               2|
+----------+-------+----------------+

üèÜ Top 3 des produits :
+--------+------+------+
| produit|    CA|Ventes|
+--------+------+------+
|  Laptop|3800.0|     3|
| Monitor| 700.0|     2|
|Keyboard| 150.0|     2|
+--------+------+------+

üë§ Client le plus actif :
+------+----------+--------+
|client|Nb_Achats|CA_Total|
+------+----------+--------+
| Alice|         4|  1650.5|
+------+----------+--------+
```

**Bar√®me d√©taill√© (4 points) :**
- 1.0 pt : CA total correct avec `sum()` et `agg()`
- 1.0 pt : Ventes par ville avec `groupBy()` et tri correct
- 1.0 pt : Top 3 produits avec `limit(3)`
- 1.0 pt : Client le plus actif avec bon crit√®re de tri

**Crit√®res de qualit√© :**
- Utilisation de `alias()` pour renommer les colonnes ‚úÖ
- Tri d√©croissant avec `desc()` ‚úÖ
- Format d'affichage lisible ‚úÖ

---

#### Exercice 1.3 : Transformations avanc√©es (25 min) - 5 points

**√ânonc√© :**
> Enrichissez les donn√©es :
> 1. Ajoutez une colonne "cat√©gorie" (Informatique/P√©riph√©rique)
> 2. Calculez la TVA (20%)
> 3. Filtrez les ventes > 100‚Ç¨
> 4. Cr√©ez une colonne "p√©riode" (Semaine 1/2/3)

**Solution compl√®te :**

```python
from pyspark.sql.functions import when, col, round, weekofyear, concat, lit

# 1. Ajout de la cat√©gorie
df_enrichi = df.withColumn(
    "categorie",
    when(col("produit") == "Laptop", "Informatique")
    .when(col("produit") == "Monitor", "Informatique")
    .otherwise("Peripherique")
)

# 2. Calcul de la TVA
df_enrichi = df_enrichi.withColumn(
    "montant_ttc",
    round(col("montant") * 1.20, 2)
).withColumn(
    "tva",
    round(col("montant") * 0.20, 2)
)

# 3. Filtre sur les montants > 100‚Ç¨
df_filtered = df_enrichi.filter(col("montant") > 100)

# 4. Ajout de la p√©riode (bas√© sur la date)
df_final = df_filtered.withColumn(
    "periode",
    concat(
        lit("Semaine "),
        weekofyear(col("date"))
    )
)

# Affichage
print("üìã Donn√©es enrichies et filtr√©es :")
df_final.select(
    "transaction_id", "produit", "montant", 
    "montant_ttc", "tva", "categorie", "periode"
).show(truncate=False)

# Statistiques par cat√©gorie
print("\nüìä Statistiques par cat√©gorie :")
df_final.groupBy("categorie") \
    .agg(
        count("*").alias("Nb_Produits"),
        sum("montant").alias("CA_HT"),
        sum("montant_ttc").alias("CA_TTC")
    ) \
    .show()
```

**R√©sultat attendu :**
```
üìã Donn√©es enrichies et filtr√©es :
+--------------+--------+-------+-----------+------+-------------+---------+
|transaction_id|produit |montant|montant_ttc|tva   |categorie    |periode  |
+--------------+--------+-------+-----------+------+-------------+---------+
|TXN001        |Laptop  |1200.0 |1440.0     |240.0 |Informatique |Semaine 3|
|TXN004        |Monitor |350.0  |420.0      |70.0  |Informatique |Semaine 3|
|TXN005        |Laptop  |1200.0 |1440.0     |240.0 |Informatique |Semaine 3|
|TXN007        |Laptop  |1400.0 |1680.0     |280.0 |Informatique |Semaine 3|
|TXN009        |Monitor |350.0  |420.0      |70.0  |Informatique |Semaine 3|
+--------------+--------+-------+-----------+------+-------------+---------+

üìä Statistiques par cat√©gorie :
+-------------+-----------+------+-------+
|    categorie|Nb_Produits| CA_HT| CA_TTC|
+-------------+-----------+------+-------+
|Informatique|          5|4500.0|5400.0|
+-------------+-----------+------+-------+
```

**Bar√®me d√©taill√© (5 points) :**
- 1.0 pt : Colonne cat√©gorie avec `when().otherwise()`
- 1.5 pt : Calcul TVA et montant TTC (avec `round()`)
- 1.0 pt : Filtre correct avec `filter()`
- 1.0 pt : Colonne p√©riode avec `weekofyear()` ou logique √©quivalente
- 0.5 pt : Affichage clair et statistiques pertinentes

**Erreurs fr√©quentes :**
- ‚ùå Oublier `round()` pour la TVA ‚Üí r√©sultats avec trop de d√©cimales
- ‚ùå Utiliser `where()` au lieu de `filter()` ‚Üí acceptable mais moins Spark idiomatique
- ‚ùå Ne pas cha√Æner les `withColumn()` ‚Üí code verbeux

---

### PARTIE 2 : Apache HBase (60 minutes)

#### Exercice 2.1 : Conception du sch√©ma (10 min) - 2 points

**√ânonc√© :**
> Concevez une table HBase pour stocker des profils clients avec :
> - Informations personnelles (nom, email, t√©l√©phone)
> - Historique d'achats (date, montant, produit)
> - Pr√©f√©rences (newsletter, langue, th√®me)

**Solution et justification :**

```
Table : clients_profiles
Row Key : client_id (ex: "C001", "C002")

Column Families :
‚îú‚îÄ info:           (Informations personnelles, rarement modifi√©es)
‚îÇ   ‚îú‚îÄ nom
‚îÇ   ‚îú‚îÄ email
‚îÇ   ‚îú‚îÄ telephone
‚îÇ   ‚îî‚îÄ date_inscription
‚îÇ
‚îú‚îÄ achats:         (Historique transactionnel, fr√©quemment mis √† jour)
‚îÇ   ‚îú‚îÄ 2024-01-15_TXN001
‚îÇ   ‚îú‚îÄ 2024-01-16_TXN002
‚îÇ   ‚îî‚îÄ ... (format: date_transaction_id)
‚îÇ
‚îî‚îÄ prefs:          (Pr√©f√©rences, mises √† jour occasionnelles)
    ‚îú‚îÄ newsletter
    ‚îú‚îÄ langue
    ‚îî‚îÄ theme
```

**Justification du design :**
1. **Row Key = client_id** : Acc√®s direct par client (O(1))
2. **3 Column Families** : S√©paration logique et optimisation
3. **Colonnes dynamiques dans "achats:"** : Scalabilit√© illimit√©e
4. **Timestamps automatiques** : Historique complet des versions

**Bar√®me (2 points) :**
- 0.5 pt : Row key pertinent (client_id)
- 1.0 pt : Column families logiquement s√©par√©es (minimum 2)
- 0.5 pt : Justification des choix

---

#### Exercice 2.2 : Impl√©mentation HBase (25 min) - 5 points

**√ânonc√© :**
> Cr√©ez la table et ins√©rez 5 profils clients avec leur historique.

**Solution compl√®te :**

```python
import happybase

# Connexion √† HBase
connection = happybase.Connection('localhost', port=9090)
print("‚úÖ Connexion HBase √©tablie")

# Cr√©ation de la table
table_name = 'clients_profiles'

# Suppression si existe (pour r√©initialiser)
if table_name.encode() in connection.tables():
    print(f"‚ö†Ô∏è  Table {table_name} existe d√©j√†, suppression...")
    connection.delete_table(table_name, disable=True)

# Cr√©ation avec 3 column families
connection.create_table(
    table_name,
    {
        'info': dict(),      # Infos personnelles
        'achats': dict(),    # Historique achats
        'prefs': dict()      # Pr√©f√©rences
    }
)
print(f"‚úÖ Table {table_name} cr√©√©e")

# Acc√®s √† la table
table = connection.table(table_name)

# Insertion de 5 clients avec historique
clients_data = [
    {
        'row_key': 'C001',
        'info:nom': 'Alice Dupont',
        'info:email': 'alice@email.com',
        'info:telephone': '0601020304',
        'info:date_inscription': '2023-06-15',
        'achats:2024-01-15_TXN001': '1200.00|Laptop',
        'achats:2024-01-16_TXN003': '75.00|Keyboard',
        'achats:2024-01-17_TXN006': '25.50|Mouse',
        'prefs:newsletter': 'true',
        'prefs:langue': 'fr',
        'prefs:theme': 'dark'
    },
    {
        'row_key': 'C002',
        'info:nom': 'Bob Martin',
        'info:email': 'bob@email.com',
        'info:telephone': '0602030405',
        'info:date_inscription': '2023-08-20',
        'achats:2024-01-15_TXN002': '25.50|Mouse',
        'achats:2024-01-17_TXN005': '1200.00|Laptop',
        'achats:2024-01-18_TXN008': '75.00|Keyboard',
        'prefs:newsletter': 'false',
        'prefs:langue': 'fr',
        'prefs:theme': 'light'
    },
    {
        'row_key': 'C003',
        'info:nom': 'Charlie Lefebvre',
        'info:email': 'charlie@email.com',
        'info:telephone': '0603040506',
        'info:date_inscription': '2023-09-10',
        'achats:2024-01-16_TXN004': '350.00|Monitor',
        'achats:2024-01-19_TXN010': '30.00|Mouse',
        'prefs:newsletter': 'true',
