# TP2 : Big Data avec Spark, HBase et Hive
## Guide √âtudiant

**√âcole Nationale des Sciences Appliqu√©es d'Oujda (ENSAO)**  
**Dur√©e** : 3 heures  
**Objectifs** : Ma√Ætriser les op√©rations de base sur Spark, HBase et Hive

---

## üìã Table des Mati√®res

1. [Pr√©requis](#pr√©requis)
2. [D√©marrage de l'environnement](#d√©marrage)
3. [Partie 1 : Apache Spark (1h)](#partie-1-spark)
4. [Partie 2 : Apache HBase (1h)](#partie-2-hbase)
5. [Partie 3 : Apache Hive (1h)](#partie-3-hive)
6. [Rendu](#rendu)

---

## üéØ Pr√©requis

### Connaissances requises
- Programmation Python de base
- Concepts SQL
- Notions de Big Data (vu en cours)

### Installation

**Sous Windows :**
```powershell
# 1. Cloner le d√©p√¥t
git clone https://github.com/omsefraoui/hadoop-spark-project.git
cd hadoop-spark-project

# 2. D√©marrer le conteneur
docker-compose up -d

# 3. Acc√©der au conteneur
docker exec -it hadoop-master bash
```

**Sous Linux/Mac :**
```bash
# M√™me chose
docker-compose up -d
docker exec -it hadoop-master bash
```

---

## üöÄ D√©marrage de l'environnement {#d√©marrage}

Une fois dans le conteneur, v√©rifiez que tous les services sont d√©marr√©s :

```bash
# V√©rifier les services
jps

# Vous devriez voir : NameNode, DataNode, HMaster, HRegionServer, etc.
```

**URLs des interfaces Web :**
- HDFS NameNode : http://localhost:9870
- YARN ResourceManager : http://localhost:8088
- Spark History : http://localhost:18080
- HBase Master : http://localhost:16010

---

## üìä Partie 1 : Apache Spark (1h) {#partie-1-spark}

### Objectifs
- Manipuler des RDDs
- Utiliser les DataFrames
- Ex√©cuter des requ√™tes Spark SQL
- Int√©grer Spark avec HDFS

### Exercice 1.1 : Premier programme Spark (15 min)

**Contexte :** Vous allez cr√©er votre premier programme PySpark pour analyser des donn√©es.

**Cr√©ez le fichier** `ex1_spark_intro.py` :

```python
from pyspark.sql import SparkSession

# Cr√©er une session Spark
spark = SparkSession.builder \
    .appName("Exercice 1.1") \
    .master("local[*]") \
    .getOrCreate()

# Cr√©er un RDD simple
data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
rdd = spark.sparkContext.parallelize(data)

# TODO 1: Calculer la somme de tous les nombres
total = rdd.reduce(lambda a, b: a + b)
print(f"Somme : {total}")

# TODO 2: Filtrer les nombres pairs
pairs = rdd.filter(lambda x: x % 2 == 0)
print(f"Nombres pairs : {pairs.collect()}")

# TODO 3: Calculer le carr√© de chaque nombre
squares = rdd.map(lambda x: x ** 2)
print(f"Carr√©s : {squares.collect()}")

# TODO 4: Calculer la moyenne
count = rdd.count()
average = total / count
print(f"Moyenne : {average}")

spark.stop()
```

**Ex√©cutez :**
```bash
python3 ex1_spark_intro.py
```

**Questions (√† inclure dans votre rapport) :**
1. Quelle est la diff√©rence entre `map()` et `filter()` ?
2. Pourquoi utilise-t-on `collect()` ?
3. Que fait la fonction `reduce()` ?

---

### Exercice 1.2 : DataFrames et Analyse de Donn√©es (25 min)

**Contexte :** Analyse des ventes d'une entreprise.

**Cr√©ez le fichier de donn√©es** `ventes.csv` :
```csv
produit,quantite,prix,region
Ordinateur,10,800,Nord
Telephone,25,300,Sud
Tablette,15,400,Est
Ordinateur,8,800,Sud
Telephone,30,300,Nord
Tablette,20,400,Ouest
Ordinateur,12,800,Est
```

**Cr√©ez** `ex2_spark_dataframe.py` :

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg

spark = SparkSession.builder.appName("Ventes").getOrCreate()
df = spark.read.csv("ventes.csv", header=True, inferSchema=True)

# TODO 1: Afficher le sch√©ma
df.printSchema()
df.show()

# TODO 2: CA par produit
ca_produit = df.groupBy("produit").agg(sum(col("quantite") * col("prix")).alias("CA"))
ca_produit.show()

# TODO 3: Meilleure r√©gion
df.groupBy("region").agg(sum("quantite").alias("total")).orderBy(col("total").desc()).show()

spark.stop()
```

**Questions :**
1. Quel produit g√©n√®re le plus de CA ?
2. Quelle r√©gion ach√®te le plus ?

---

### Exercice 1.3 : Spark SQL (20 min)

**Cr√©ez** `ex3_spark_sql.py` :

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SQL").getOrCreate()
df = spark.read.csv("ventes.csv", header=True, inferSchema=True)

# Cr√©er une vue temporaire
df.createOrReplaceTempView("ventes")

# TODO 1: Requ√™te SQL - CA total
