#!/bin/bash
# Script de d√©marrage de tous les services Big Data
# Usage: ./start-all-services.sh

set -e

echo "=========================================="
echo "üöÄ D√âMARRAGE CLUSTER BIG DATA"
echo "=========================================="
echo ""

# Couleurs pour l'affichage
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Fonctions de log
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

# V√©rifier que nous sommes dans le conteneur (Hadoop pr√©sent)
if [ -z "$HADOOP_HOME" ] || [ ! -d "$HADOOP_HOME" ]; then
    echo "‚ùå Erreur: HADOOP_HOME non d√©fini ou invalide. √ätes-vous dans le conteneur ?"
    exit 1
fi

# R√©pertoire du NameNode (align√© avec les logs Hadoop)
NAME_DIR="$HADOOP_HOME/tmp/dfs/name/current"

# Fonction utilitaire : tester si un NameNode tourne d√©j√†
is_namenode_running() {
    jps | grep -q "NameNode"
}

echo ""
log_info "Configuration d√©tect√©e :"
echo "   HADOOP_HOME = $HADOOP_HOME"
echo "   HBASE_HOME  = ${HBASE_HOME:-<non d√©fini>}"
echo "   HIVE_HOME   = ${HIVE_HOME:-<non d√©fini>}"
echo "   SPARK_HOME  = ${SPARK_HOME:-<non d√©fini>}"
echo ""

# 1Ô∏è‚É£  D√©marrer SSH
log_info "1Ô∏è‚É£  D√©marrage SSH..."
service ssh start || log_warn "   ‚ö†Ô∏è  Impossible de d√©marrer SSH (peut-√™tre d√©j√† lanc√©)."
sleep 2

# 2Ô∏è‚É£  Formatage NameNode si n√©cessaire (UNE SEULE FOIS)
if is_namenode_running; then
    log_warn "2Ô∏è‚É£  NameNode d√©j√† en cours d'ex√©cution, on saute le formatage."
elif [ ! -d "$NAME_DIR" ]; then
    log_info "2Ô∏è‚É£  Formatage NameNode (premi√®re utilisation)..."
    "$HADOOP_HOME/bin/hdfs" namenode -format -force -nonInteractive
else
    log_info "2Ô∏è‚É£  NameNode d√©j√† format√©, on continue..."
fi


# Pr√©parer les r√©pertoires locaux HDFS (NameNode / DataNode)
DATA_DIR="$HADOOP_HOME/tmp/dfs/data"
NAME_DIR="$HADOOP_HOME/tmp/dfs/name"

log_info "   Pr√©paration des r√©pertoires HDFS locaux..."
mkdir -p "$DATA_DIR" "$NAME_DIR"
chown -R root:root "$HADOOP_HOME/tmp" || true
log_info "   ‚úÖ R√©pertoires HDFS locaux pr√™ts"


# 3Ô∏è‚É£  D√©marrage Hadoop HDFS
log_info "3Ô∏è‚É£  D√©marrage Hadoop HDFS (NameNode + DataNode)..."
"$HADOOP_HOME/sbin/start-dfs.sh"

sleep 5

# V√©rifier HDFS
if jps | grep -q "NameNode"; then
    log_info "   ‚úÖ NameNode d√©marr√©"
else
    log_warn "   ‚ö†Ô∏è  NameNode non d√©tect√©"
fi

if jps | grep -q "DataNode"; then
    log_info "   ‚úÖ DataNode d√©marr√©"
else
    log_warn "   ‚ö†Ô∏è  DataNode non d√©tect√©"
fi

# 4Ô∏è‚É£  D√©marrage YARN
log_info "4Ô∏è‚É£  D√©marrage YARN..."
"$HADOOP_HOME/sbin/start-yarn.sh"
sleep 5

if jps | grep -q "ResourceManager"; then
    log_info "   ‚úÖ ResourceManager d√©marr√©"
else
    log_warn "   ‚ö†Ô∏è  ResourceManager non d√©tect√©"
fi

if jps | grep -q "NodeManager"; then
    log_info "   ‚úÖ NodeManager d√©marr√©"
else
    log_warn "   ‚ö†Ô∏è  NodeManager non d√©tect√©"
fi

# 5Ô∏è‚É£  Cr√©ation des r√©pertoires HDFS pour Hive et Spark...
log_info "5Ô∏è‚É£  Cr√©ation des r√©pertoires HDFS pour Hive et Spark..."

# Chemins simples (fs.defaultFS)
"$HADOOP_HOME/bin/hdfs" dfs -mkdir -p /tmp || true
"$HADOOP_HOME/bin/hdfs" dfs -mkdir -p /user/hive/warehouse || true
"$HADOOP_HOME/bin/hdfs" dfs -mkdir -p /spark-logs || true

"$HADOOP_HOME/bin/hdfs" dfs -chmod g+w /tmp || true
"$HADOOP_HOME/bin/hdfs" dfs -chmod g+w /user/hive/warehouse || true
"$HADOOP_HOME/bin/hdfs" dfs -chmod 777 /spark-logs || true

# Chemin complet utilis√© par Spark pour les event logs
"$HADOOP_HOME/bin/hdfs" dfs -mkdir -p "hdfs://spark-master:9000/spark-logs" || true
"$HADOOP_HOME/bin/hdfs" dfs -chmod 777 "hdfs://spark-master:9000/spark-logs" || true

log_info "   ‚úÖ R√©pertoires Hive et Spark cr√©√©s / v√©rifi√©s"

# 6Ô∏è‚É£  ZooKeeper + HBase
if [ -z "$HBASE_HOME" ] || [ ! -d "$HBASE_HOME" ]; then
    log_warn "6Ô∏è‚É£  HBASE_HOME non d√©fini, ZooKeeper/HBase ne seront pas d√©marr√©s."
else
    HBASE_ENV="$HBASE_HOME/conf/hbase-env.sh"

    # 6.1 D√©marrage ZooKeeper si non g√©r√© automatiquement par HBase
    if [ -f "$HBASE_ENV" ] && grep -q "^[[:space:]]*export[[:space:]]\+HBASE_MANAGES_ZK *= *true" "$HBASE_ENV" 2>/dev/null; then
        log_info "6Ô∏è‚É£  HBASE_MANAGES_ZK=true : ZooKeeper sera d√©marr√© par HBase."
    else
        log_info "6Ô∏è‚É£  D√©marrage ZooKeeper pour HBase..."
        "$HBASE_HOME/bin/hbase-daemon.sh" start zookeeper || log_warn "   ‚ö†Ô∏è  Probl√®me au d√©marrage de ZooKeeper (peut-√™tre d√©j√† lanc√©)."
        sleep 3
    fi

    # 7Ô∏è‚É£  D√©marrage HBase (HMaster + RegionServer)
    log_info "7Ô∏è‚É£  D√©marrage HBase..."
    "$HBASE_HOME/bin/start-hbase.sh"
    sleep 5

    if jps | grep -q "HMaster"; then
        log_info "   ‚úÖ HBase Master d√©marr√©"
    else
        log_warn "   ‚ö†Ô∏è  HBase Master non d√©tect√©"
    fi

    if jps | grep -q "HRegionServer"; then
        log_info "   ‚úÖ HBase RegionServer d√©marr√©"
    else
        log_warn "   ‚ö†Ô∏è  HBase RegionServer non d√©tect√©"
    fi

    # 8Ô∏è‚É£  D√©marrage HBase Thrift Server (pour les clients Python, etc.)
    log_info "8Ô∏è‚É£  D√©marrage HBase Thrift Server..."
    "$HBASE_HOME/bin/hbase-daemon.sh" start thrift || log_warn "   ‚ö†Ô∏è  √âchec (ou d√©j√† d√©marr√©) pour Thrift."
    sleep 3
    log_info "   ‚úÖ Thrift Server suppos√© d√©marr√© (port 9090)"
fi

# 9Ô∏è‚É£  Initialisation et services Hive
if [ -z "$HIVE_HOME" ] || [ ! -d "$HIVE_HOME" ]; then
    log_warn "9Ô∏è‚É£  HIVE_HOME non d√©fini, Hive ne sera pas initialis√©."
else
    log_info "9Ô∏è‚É£  Initialisation du Metastore Hive (si n√©cessaire)..."
    if [ ! -d "$HIVE_HOME/metastore_db" ]; then
        cd "$HIVE_HOME"
        "$HIVE_HOME/bin/schematool" -dbType derby -initSchema
        log_info "   ‚úÖ Sch√©ma Hive initialis√©"
    else
        log_info "   ‚úÖ Sch√©ma Hive d√©j√† existant"
    fi

    # üîü D√©marrage Hive Metastore
    log_info "üîü D√©marrage Hive Metastore..."
    nohup "$HIVE_HOME/bin/hive" --service metastore > /var/log/hive-metastore.log 2>&1 &
    sleep 5
    log_info "   ‚úÖ Metastore d√©marr√©"

    # 1Ô∏è‚É£1Ô∏è‚É£  D√©marrage HiveServer2
    log_info "1Ô∏è‚É£1Ô∏è‚É£  D√©marrage HiveServer2..."
    nohup "$HIVE_HOME/bin/hive" --service hiveserver2 > /var/log/hive-server2.log 2>&1 &
    sleep 5
    log_info "   ‚úÖ HiveServer2 d√©marr√© (port 10000)"
fi

# 1Ô∏è‚É£2Ô∏è‚É£  D√©marrage Spark (Master + History Server)
# Si SPARK_HOME n'est pas d√©fini, essayer de le d√©duire √† partir de spark-shell
if [ -z "$SPARK_HOME" ] || [ ! -d "$SPARK_HOME" ]; then
    if command -v spark-shell >/dev/null 2>&1; then
        SPARK_BIN="$(command -v spark-shell)"
        SPARK_HOME="$(dirname "$(dirname "$SPARK_BIN")")"
        log_info "1Ô∏è‚É£2Ô∏è‚É£  SPARK_HOME d√©duit automatiquement : $SPARK_HOME"
    else
        log_warn "1Ô∏è‚É£2Ô∏è‚É£  SPARK_HOME non d√©fini et spark-shell introuvable, Spark ne sera pas d√©marr√©."
    fi
fi

if [ -n "$SPARK_HOME" ] && [ -d "$SPARK_HOME" ]; then
    log_info "1Ô∏è‚É£2Ô∏è‚É£  D√©marrage Spark Master..."
    "$SPARK_HOME/sbin/start-master.sh" || log_warn "   ‚ö†Ô∏è  √âchec au d√©marrage du Spark Master."
    sleep 3

    if jps | grep -q "Master"; then
        log_info "   ‚úÖ Spark Master d√©marr√© (port 8080)"
    else
        log_warn "   ‚ö†Ô∏è  Spark Master non d√©tect√©"
    fi

    log_info "1Ô∏è‚É£3Ô∏è‚É£  D√©marrage Spark History Server..."
    "$SPARK_HOME/sbin/start-history-server.sh" || log_warn "   ‚ö†Ô∏è  History Server non d√©marr√© (v√©rifier les logs)."
    sleep 2
    log_info "   ‚úÖ History Server demand√© (port 18080)"
else
    log_warn "1Ô∏è‚É£2Ô∏è‚É£  SPARK_HOME invalide, Spark ne sera pas d√©marr√©."
fi

echo ""
echo "=========================================="
echo "‚úÖ TOUS LES SERVICES ONT √âT√â LANC√âS"
echo "=========================================="
echo ""

# Afficher les services actifs
log_info "Services Java actifs (jps) :"
jps

echo ""
log_info "üåê URLs des interfaces Web (dans le conteneur) :"
echo "   Hadoop NameNode:    http://localhost:9870"
echo "   YARN ResourceMgr:   http://localhost:8088"
echo "   Spark Master:       http://localhost:8080"
echo "   Spark History:      http://localhost:18080"
echo "   HBase Master:       http://localhost:16010"
echo ""

log_info "üîå Ports des services :"
echo "   HDFS NameNode:      9000"
echo "   HBase Thrift:       9090"
echo "   HiveServer2:        10000"
echo "   Spark Master:       7077"
echo ""

log_info "üìù Pour tester rapidement :"
echo "   - hdfs dfs -ls /"
echo "   - hive          (CREATE TABLE / SELECT)"
echo "   - hbase shell   (create/get)"
echo "   - spark-shell   (RDD simple)"
echo ""
