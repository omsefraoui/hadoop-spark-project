#!/bin/bash
# Script de d√©marrage de tous les services Big Data
# Usage: ./start-all-services.sh

set -e

echo "=========================================="
echo "üöÄ D√âMARRAGE CLUSTER BIG DATA"
echo "=========================================="
echo ""

# Couleurs pour l'affichage
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

log_info()  { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn()  { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERREUR]${NC} $1"; }

# V√©rifier les variables d'environnement
if [ -z "$HADOOP_HOME" ] || [ ! -d "$HADOOP_HOME" ]; then
    log_error "HADOOP_HOME non d√©fini ou invalide. √ätes-vous dans le conteneur spark-master ?"
    exit 1
fi

# R√©pertoires HDFS locaux
NAME_DIR="$HADOOP_HOME/tmp/dfs/name/current"
DATA_DIR="$HADOOP_HOME/tmp/dfs/data/current"

log_info "HADOOP_HOME = $HADOOP_HOME"
[ -n "$HBASE_HOME" ] && log_info "HBASE_HOME  = $HBASE_HOME" || log_warn "HBASE_HOME non d√©fini"
[ -n "$HIVE_HOME" ]  && log_info "HIVE_HOME   = $HIVE_HOME"  || log_warn "HIVE_HOME non d√©fini"
[ -n "$SPARK_HOME" ] && log_info "SPARK_HOME  = $SPARK_HOME" || log_warn "SPARK_HOME non d√©fini"

echo ""

############################
# 1. SSH
############################
log_info "1Ô∏è‚É£  D√©marrage SSH..."
service ssh start >/dev/null 2>&1 || log_warn "SSH d√©j√† d√©marr√© ?"
sleep 2

############################
# 2. Formatage NameNode si n√©cessaire
############################
if [ ! -d "$NAME_DIR" ]; then
    log_info "2Ô∏è‚É£  Formatage NameNode (premi√®re utilisation)..."
    "$HADOOP_HOME/bin/hdfs" namenode -format -force -nonInteractive
else
    log_info "2Ô∏è‚É£  NameNode d√©j√† format√©, on continue..."
fi

############################
# 3. HDFS (NameNode + DataNode)
############################
log_info "3Ô∏è‚É£  D√©marrage Hadoop HDFS (NameNode + DataNode)..."
"$HADOOP_HOME/sbin/start-dfs.sh"
sleep 5

# V√©rifications HDFS
if jps | grep -q "NameNode"; then
    log_info "   ‚úÖ NameNode d√©marr√©"
else
    log_warn "   ‚ö†Ô∏è  NameNode non d√©tect√©"
fi

if jps | grep -q "DataNode"; then
    log_info "   ‚úÖ DataNode d√©marr√©"
else
    log_warn "   ‚ö†Ô∏è  DataNode non d√©tect√© ‚Äì v√©rifier les logs dans $HADOOP_HOME/logs/*datanode*.log"
fi

############################
# 4. YARN (ResourceManager + NodeManagers distants)
############################
log_info "4Ô∏è‚É£  D√©marrage YARN (ResourceManager + NodeManagers)..."
"$HADOOP_HOME/sbin/start-yarn.sh"
sleep 5

if jps | grep -q "ResourceManager"; then
    log_info "   ‚úÖ ResourceManager d√©marr√©"
else
    log_warn "   ‚ö†Ô∏è  ResourceManager non d√©tect√©"
fi

# 4bis. NodeManager local sur spark-master (important pour Spark/YARN)
log_info "4Ô∏è‚É£bis  D√©marrage NodeManager local sur le master..."
"$HADOOP_HOME/bin/yarn" --daemon start nodemanager || \
  log_warn "   ‚ö†Ô∏è  NodeManager local d√©j√† d√©marr√© ?"
sleep 3

# Afficher la liste des n≈ìuds YARN
log_info "   üîé N≈ìuds YARN enregistr√©s :"
yarn node -list || log_warn "   ‚ö†Ô∏è  Impossible de lister les n≈ìuds YARN (v√©rifier ResourceManager)."

############################
# 5. Pr√©paration HDFS (Hive + Spark logs)
############################
log_info "5Ô∏è‚É£  Cr√©ation des r√©pertoires HDFS n√©cessaires (Hive + Spark)..."
# R√©pertoires Hive
"$HADOOP_HOME/bin/hdfs" dfs -mkdir -p /tmp                || true
"$HADOOP_HOME/bin/hdfs" dfs -mkdir -p /user/hive/warehouse || true
"$HADOOP_HOME/bin/hdfs" dfs -chmod g+w /tmp                || true
"$HADOOP_HOME/bin/hdfs" dfs -chmod g+w /user/hive/warehouse || true
log_info "   ‚úÖ R√©pertoires Hive cr√©√©s / v√©rifi√©s"

# R√©pertoire Spark logs (pour spark.eventLog.dir = hdfs://spark-master:9000/spark-logs)
"$HADOOP_HOME/bin/hdfs" dfs -mkdir -p /spark-logs || true
"$HADOOP_HOME/bin/hdfs" dfs -chmod 1777 /spark-logs || true
log_info "   ‚úÖ R√©pertoire /spark-logs cr√©√© / v√©rifi√© dans HDFS"

############################
# 6. ZooKeeper (option HBase embarqu√©)
############################
if [ -n "$HBASE_HOME" ] && [ -d "$HBASE_HOME" ]; then
    log_info "6Ô∏è‚É£  D√©marrage ZooKeeper (via HBase)..."
    "$HBASE_HOME/bin/hbase-daemon.sh" start zookeeper || \
      log_warn "   ‚ö†Ô∏è  ZooKeeper d√©j√† d√©marr√© ou non disponible via HBase."
    sleep 3
else
    log_warn "6Ô∏è‚É£  HBASE_HOME non d√©fini : ZooKeeper non d√©marr√©."
fi

############################
# 7. HBase (Master + RegionServer)
############################
if [ -n "$HBASE_HOME" ] && [ -d "$HBASE_HOME" ]; then
    log_info "7Ô∏è‚É£  D√©marrage HBase..."
    "$HBASE_HOME/bin/start-hbase.sh"
    sleep 5

    if jps | grep -q "HMaster"; then
        log_info "   ‚úÖ HBase Master d√©marr√©"
    else
        log_warn "   ‚ö†Ô∏è  HBase Master non d√©tect√©"
    fi

    ############################
    # 8. Thrift HBase (pour Python, etc.)
    ############################
    log_info "8Ô∏è‚É£  D√©marrage HBase Thrift Server..."
    "$HBASE_HOME/bin/hbase-daemon.sh" start thrift || \
      log_warn "   ‚ö†Ô∏è  Thrift d√©j√† d√©marr√© ?"
    sleep 3
    log_info "   ‚úÖ Thrift Server d√©marr√© (port 9090)"
else
    log_warn "7Ô∏è‚É£  HBase/Thrift non d√©marr√©s (HBASE_HOME non d√©fini)."
fi

############################
# 9. Hive (Metastore + HiveServer2)
############################
if [ -n "$HIVE_HOME" ] && [ -d "$HIVE_HOME" ]; then
    log_info "9Ô∏è‚É£  Initialisation Metastore Hive (si n√©cessaire)..."
    if [ ! -d "$HIVE_HOME/metastore_db" ]; then
        cd "$HIVE_HOME"
        "$HIVE_HOME/bin/schematool" -dbType derby -initSchema
        log_info "   ‚úÖ Schema Hive initialis√©"
    else
        log_info "   ‚úÖ Schema Hive d√©j√† existant"
    fi

    log_info "üîü D√©marrage Hive Metastore..."
    nohup "$HIVE_HOME/bin/hive" --service metastore > /var/log/hive-metastore.log 2>&1 &
    sleep 5
    log_info "   ‚úÖ Metastore d√©marr√©"

    log_info "1Ô∏è‚É£1Ô∏è‚É£  D√©marrage HiveServer2..."
    nohup "$HIVE_HOME/bin/hive" --service hiveserver2 > /var/log/hive-server2.log 2>&1 &
    sleep 5
    log_info "   ‚úÖ HiveServer2 d√©marr√© (port 10000)"
else
    log_warn "9Ô∏è‚É£  Hive non d√©marr√© (HIVE_HOME non d√©fini)."
fi

############################
# 10. Spark (Master + HistoryServer)
############################
if [ -n "$SPARK_HOME" ] && [ -d "$SPARK_HOME" ]; then
    log_info "1Ô∏è‚É£2Ô∏è‚É£  D√©marrage Spark Master..."
    "$SPARK_HOME/sbin/start-master.sh"
    sleep 3

    if jps | grep -q "Master"; then
        log_info "   ‚úÖ Spark Master d√©marr√© (web UI 8080, port 7077)"
    else
        log_warn "   ‚ö†Ô∏è  Spark Master non d√©tect√©"
    fi

    log_info "1Ô∏è‚É£3Ô∏è‚É£  D√©marrage Spark History Server..."
    "$SPARK_HOME/sbin/start-history-server.sh" || \
      log_warn "   ‚ö†Ô∏è  History Server d√©j√† d√©marr√© ?"
    sleep 3
    log_info "   ‚úÖ History Server d√©marr√© (port 18080)"
else
    log_warn "1Ô∏è‚É£2Ô∏è‚É£  Spark non d√©marr√© (SPARK_HOME non d√©fini)."
fi

############################
# R√©capitulatif
############################
echo ""
echo "=========================================="
echo "‚úÖ TOUS LES SERVICES ONT √âT√â LANC√âS (dans la mesure du possible)"
echo "=========================================="
echo ""

log_info "Services Java actifs (jps) :"
jps
echo ""

log_info "üåê Interfaces Web (depuis la machine h√¥te, avec le bon port mapp√© Docker) :"
echo "   Hadoop NameNode:    http://localhost:9870"
echo "   YARN ResourceMgr:   http://localhost:8088"
echo "   NodeManager:        http://localhost:8042"
echo "   Spark Master:       http://localhost:8080"
echo "   Spark History:      http://localhost:18080"
echo "   HBase Master:       http://localhost:16010"
echo ""

log_info "üîå Ports principaux des services :"
echo "   HDFS NameNode:      9000"
echo "   HBase Thrift:       9090"
echo "   HiveServer2:        10000"
echo "   Spark Master:       7077"
echo ""

log_info "üìù Pour tester Spark sur YARN :"
echo "   hdfs dfs -ls /"
echo "   spark-submit --master yarn /scripts/wordcount.py"
echo ""
