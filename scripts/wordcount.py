#!/usr/bin/env python3
"""
TP2 - Partie 2 : Word Count avec Spark
Programme de comptage de mots utilisant les RDD Spark
"""

from pyspark import SparkContext, SparkConf
import sys

def main():
    # Configuration Spark
    conf = SparkConf().setAppName("WordCount-TP2")
    sc = SparkContext(conf=conf)
    
    try:
        print("=" * 60)
        print("WORD COUNT - Traitement Big Data avec Spark")
        print("=" * 60)
        
        # Chemin du fichier dans HDFS
        input_path = "hdfs://spark-master:9000/user/spark/data/spark_data.txt"
        
        print(f"\nğŸ“– Lecture du fichier : {input_path}")
        
        # Lecture du fichier
        lines = sc.textFile(input_path)
        
        # Transformations
        print("\nğŸ”„ Application des transformations...")
        
        # 1. flatMap : DÃ©couper chaque ligne en mots
        words = lines.flatMap(lambda line: line.split())
        
        # 2. map : CrÃ©er des paires (mot, 1)
        word_pairs = words.map(lambda word: (word, 1))
        
        # 3. reduceByKey : AgrÃ©ger les comptages
        word_counts = word_pairs.reduceByKey(lambda a, b: a + b)
        
        # Action : Collecter les rÃ©sultats
        print("\nğŸ“Š Collecte des rÃ©sultats...")
        results = word_counts.collect()
        
        # Trier par frÃ©quence dÃ©croissante
        sorted_results = sorted(results, key=lambda x: -x[1])
        
        # Affichage des rÃ©sultats
        print("\n" + "=" * 60)
        print("RÃ‰SULTATS DU COMPTAGE")
        print("=" * 60)
        print(f"{'Mot':<30} | {'FrÃ©quence':>10}")
        print("-" * 60)
        
        for word, count in sorted_results:
            print(f"{word:<30} | {count:>10}")
        
        print("-" * 60)
        print(f"Total de mots uniques : {len(results)}")
        print(f"Total d'occurrences : {sum(count for _, count in results)}")
        print("=" * 60)
        
        # Sauvegarder les rÃ©sultats dans HDFS
        output_path = "hdfs://spark-master:9000/user/spark/output/wordcount"
        print(f"\nğŸ’¾ Sauvegarde des rÃ©sultats dans : {output_path}")
        
        # Supprimer le rÃ©pertoire de sortie s'il existe
        import subprocess
        subprocess.run(["hdfs", "dfs", "-rm", "-r", output_path], 
                      stderr=subprocess.DEVNULL)
        
        # Sauvegarder
        word_counts.saveAsTextFile(output_path)
        print("âœ… RÃ©sultats sauvegardÃ©s avec succÃ¨s !")
        
    except Exception as e:
        print(f"\nâŒ ERREUR : {str(e)}", file=sys.stderr)
        sys.exit(1)
    finally:
        # ArrÃªt du SparkContext
        sc.stop()
        print("\nğŸ Fin du traitement\n")

if __name__ == "__main__":
    main()
