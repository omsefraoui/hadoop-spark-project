# Dockerfile Optimisé pour Cluster Hadoop + Spark + Hive + HBase
# Version 2.0 - Avec gestion robuste des téléchargements

FROM ubuntu:20.04

LABEL maintainer="Omar Sefraoui"
LABEL description="Hadoop 3.3.6 + Spark 3.5.0 + Hive 3.1.3 + HBase 2.5.5"
LABEL version="2.0"

# Éviter les prompts interactifs
ENV DEBIAN_FRONTEND=noninteractive

# Variables pour les versions
ENV HADOOP_VERSION=3.3.6
ENV SPARK_VERSION=3.5.0
ENV HIVE_VERSION=3.1.3
ENV HBASE_VERSION=2.5.5

# Chemins d'installation
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV HADOOP_HOME=/opt/hadoop
ENV SPARK_HOME=/opt/spark
ENV HIVE_HOME=/opt/hive
ENV HBASE_HOME=/opt/hbase

# PATH
ENV PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HIVE_HOME/bin:$HBASE_HOME/bin:$PATH

# Configuration Hadoop users
ENV HDFS_NAMENODE_USER=root
ENV HDFS_DATANODE_USER=root
ENV HDFS_SECONDARYNAMENODE_USER=root
ENV YARN_RESOURCEMANAGER_USER=root
ENV YARN_NODEMANAGER_USER=root

# ============================================
# INSTALLATION DES PACKAGES DE BASE
# ============================================

RUN apt-get update && apt-get install -y --no-install-recommends \
    openjdk-8-jdk \
    wget \
    curl \
    ssh \
    rsync \
    vim \
    nano \
    net-tools \
    iputils-ping \
    python3 \
    python3-pip \
    procps \
    ca-certificates \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# ============================================
# INSTALLATION DES BIBLIOTHÈQUES PYTHON
# ============================================

RUN pip3 install --no-cache-dir --upgrade pip && \
    pip3 install --no-cache-dir \
    pyspark==3.5.0 \
    pandas \
    numpy \
    matplotlib \
    py4j

# ============================================
# TÉLÉCHARGEMENT HADOOP avec retry
# ============================================

RUN echo "Téléchargement Hadoop ${HADOOP_VERSION}..." && \
    wget --retry-connrefused --waitretry=1 --read-timeout=20 --timeout=15 -t 5 \
    https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz \
    -O hadoop.tar.gz && \
    echo "Extraction Hadoop..." && \
    tar -xzf hadoop.tar.gz && \
    mv hadoop-${HADOOP_VERSION} ${HADOOP_HOME} && \
    rm hadoop.tar.gz && \
    echo "Hadoop installé avec succès"

# ============================================
# TÉLÉCHARGEMENT SPARK avec retry
# ============================================

RUN echo "Téléchargement Spark ${SPARK_VERSION}..." && \
    wget --retry-connrefused --waitretry=1 --read-timeout=20 --timeout=15 -t 5 \
    https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz \
    -O spark.tgz && \
    echo "Extraction Spark..." && \
    tar -xzf spark.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop3 ${SPARK_HOME} && \
    rm spark.tgz && \
    echo "Spark installé avec succès"

# ============================================
# TÉLÉCHARGEMENT HIVE avec retry
# ============================================

RUN echo "Téléchargement Hive ${HIVE_VERSION}..." && \
    wget --retry-connrefused --waitretry=1 --read-timeout=20 --timeout=15 -t 5 \
    https://archive.apache.org/dist/hive/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bin.tar.gz \
    -O hive.tar.gz && \
    echo "Extraction Hive..." && \
    tar -xzf hive.tar.gz && \
    mv apache-hive-${HIVE_VERSION}-bin ${HIVE_HOME} && \
    rm hive.tar.gz && \
    echo "Hive installé avec succès"

# ============================================
# TÉLÉCHARGEMENT HBASE avec retry
# ============================================

RUN echo "Téléchargement HBase ${HBASE_VERSION}..." && \
    wget --retry-connrefused --waitretry=1 --read-timeout=20 --timeout=15 -t 5 \
    https://archive.apache.org/dist/hbase/${HBASE_VERSION}/hbase-${HBASE_VERSION}-bin.tar.gz \
    -O hbase.tar.gz && \
    echo "Extraction HBase..." && \
    tar -xzf hbase.tar.gz && \
    mv hbase-${HBASE_VERSION} ${HBASE_HOME} && \
    rm hbase.tar.gz && \
    echo "HBase installé avec succès"

# ============================================
# CONFIGURATION SSH
# ============================================

RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    echo "StrictHostKeyChecking no" >> /etc/ssh/ssh_config && \
    echo "UserKnownHostsFile /dev/null" >> /etc/ssh/ssh_config

# ============================================
# CONFIGURATION JAVA
# ============================================

RUN echo "export JAVA_HOME=${JAVA_HOME}" >> ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh && \
    echo "export JAVA_HOME=${JAVA_HOME}" >> ${SPARK_HOME}/conf/spark-env.sh

# ============================================
# COPIE DES CONFIGURATIONS
# ============================================

# Hadoop
COPY config/hadoop/core-site.xml ${HADOOP_HOME}/etc/hadoop/
COPY config/hadoop/hdfs-site.xml ${HADOOP_HOME}/etc/hadoop/
COPY config/hadoop/mapred-site.xml ${HADOOP_HOME}/etc/hadoop/
COPY config/hadoop/yarn-site.xml ${HADOOP_HOME}/etc/hadoop/
COPY config/hadoop/workers ${HADOOP_HOME}/etc/hadoop/

# Spark
COPY config/spark/spark-defaults.conf ${SPARK_HOME}/conf/
COPY config/spark/spark-env.sh ${SPARK_HOME}/conf/
RUN chmod +x ${SPARK_HOME}/conf/spark-env.sh

# Hive
COPY config/hive/hive-site.xml ${HIVE_HOME}/conf/
RUN echo "export JAVA_HOME=${JAVA_HOME}" >> ${HIVE_HOME}/conf/hive-env.sh

# HBase
COPY config/hbase/hbase-site.xml ${HBASE_HOME}/conf/
COPY config/hbase/regionservers ${HBASE_HOME}/conf/
RUN echo "export JAVA_HOME=${JAVA_HOME}" >> ${HBASE_HOME}/conf/hbase-env.sh

# ============================================
# COPIE DES SCRIPTS ET DONNÉES
# ============================================

COPY scripts/ /scripts/
RUN chmod +x /scripts/*.py

COPY data/ /data/
RUN chmod 644 /data/*

COPY start-master.sh /start-master.sh
COPY start-worker.sh /start-worker.sh
RUN chmod +x /start-master.sh /start-worker.sh

# ============================================
# CRÉATION DES RÉPERTOIRES
# ============================================

RUN mkdir -p /opt/hadoop/dfs/name && \
    mkdir -p /opt/hadoop/dfs/data && \
    mkdir -p /opt/hive/warehouse && \
    mkdir -p /opt/spark/logs && \
    mkdir -p /tmp/hive && \
    chmod -R 777 /opt/hadoop/dfs && \
    chmod -R 777 /opt/hive/warehouse && \
    chmod -R 777 /opt/spark/logs && \
    chmod -R 777 /tmp/hive

# ============================================
# TÉLÉCHARGEMENT DRIVER JDBC
# ============================================

RUN wget -q https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.24/postgresql-42.2.24.jar \
    -O ${HIVE_HOME}/lib/postgresql-jdbc.jar

# ============================================
# RÉSOLUTION CONFLITS GUAVA
# ============================================

RUN rm -f ${HIVE_HOME}/lib/guava-*.jar && \
    cp ${HADOOP_HOME}/share/hadoop/common/lib/guava-*.jar ${HIVE_HOME}/lib/

# ============================================
# EXPOSITION DES PORTS
# ============================================

# HDFS
EXPOSE 9870 9000 9864 9866

# YARN
EXPOSE 8088 8042 8030 8031 8032 8033

# Spark
EXPOSE 8080 7077 8081 4040 18080

# Hive
EXPOSE 10000 10002 9083

# HBase
EXPOSE 16010 16020 16030 2181

# SSH
EXPOSE 22

# ============================================
# VARIABLES D'ENVIRONNEMENT DANS .bashrc
# ============================================

RUN echo "export JAVA_HOME=${JAVA_HOME}" >> ~/.bashrc && \
    echo "export HADOOP_HOME=${HADOOP_HOME}" >> ~/.bashrc && \
    echo "export SPARK_HOME=${SPARK_HOME}" >> ~/.bashrc && \
    echo "export HIVE_HOME=${HIVE_HOME}" >> ~/.bashrc && \
    echo "export HBASE_HOME=${HBASE_HOME}" >> ~/.bashrc && \
    echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HIVE_HOME/bin:$HBASE_HOME/bin' >> ~/.bashrc

# ============================================
# INITIALISATION HIVE METASTORE
# ============================================

RUN ${HIVE_HOME}/bin/schematool -initSchema -dbType derby || true

# ============================================
# WORKDIR ET CMD
# ============================================

WORKDIR /opt

CMD ["/bin/bash"]
